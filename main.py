import streamlit as st
from openai import OpenAI
import anthropic
from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI
from langchain.tools import DuckDuckGoSearchRun
import requests
import logging
import uuid

# API Setup Sidebar
with st.sidebar:
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    anthropic_api_key = st.text_input("Anthropic API Key", type="password")
    st.markdown("[Get OpenAI Key](https://platform.openai.com/account/api-keys)")
    st.markdown("[Code on GitHub](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)")

# Initialize session state for messages and files
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Hello! How can I assist you?"}]
if "files" not in st.session_state:
    st.session_state["files"] = []

# Chat Interface and Message Display
st.title("üß† AI Multi-Tool Chat Interface")

def display_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

display_messages()

# Message Input Handling
if prompt := st.chat_input():
    if not openai_api_key:
        st.info("Please provide an OpenAI API key to proceed.")
        st.stop()

    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # OpenAI Chat
    client = OpenAI(api_key=openai_api_key)
    response = client.Chat.create(model="gpt-4o", messages=st.session_state["messages"])
    msg_content = response.choices[0].message.content
    st.session_state["messages"].append({"role": "assistant", "content": msg_content})
    st.chat_message("assistant").write(msg_content)

    # Additional functionalities like file rendering, if available
    if "files" in st.session_state:
        for file in st.session_state["files"]:
            st.write(f"File: {file['name']} - {file['type']}")

# Search with LangChain Integration
def chat_with_search():
    llm = ChatOpenAI(model_name="gpt-4o", openai_api_key=openai_api_key)
    search = DuckDuckGoSearchRun(name="Search")
    search_agent = initialize_agent([search], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
    response = search_agent.run(st.session_state["messages"])
    st.session_state["messages"].append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)

st.button("üîç Search Web", on_click=chat_with_search)

# File Upload and Q&A with Anthropic Integration
uploaded_file = st.file_uploader("Upload a file for Q&A")
question = st.text_input("Ask a question about the uploaded file", placeholder="Summarize the file")

if uploaded_file and question:
    if not anthropic_api_key:
        st.info("Please provide an Anthropic API key.")
        st.stop()
    article = uploaded_file.read().decode()
    client = anthropic.Client(api_key=anthropic_api_key)
    prompt = f"{anthropic.HUMAN_PROMPT} {question}\n\n{article}\n{anthropic.AI_PROMPT}"
    response = client.completions.create(prompt=prompt, model="claude-v1", max_tokens_to_sample=100)
    st.session_state["messages"].append({"role": "assistant", "content": response.completion})
    st.chat_message("assistant").write(response.completion)

# Image Generation with DALL¬∑E 3
if st.button("Generate Image with DALL¬∑E 3"):
    image_prompt = st.text_input("Enter a description for the image")
    if image_prompt:
        response = client.Image.create(prompt=image_prompt, n=1, model="dall-e-3")
        image_url = response['data'][0]['url']
        st.image(image_url, caption="Generated by DALL¬∑E 3")

# Vision API with GPT-4 Turbo Vision
if st.button("Analyze Image with GPT-4 Turbo"):
    uploaded_image = st.file_uploader("Upload an image for analysis", type=["jpg", "png"])
    if uploaded_image:
        base64_image = base64.b64encode(uploaded_image.read()).decode()
        prompt = f"What is shown in this image?"
        response = client.Chat.create(
            model="gpt-4-vision-preview",
            messages=[{"role": "user", "content": prompt}],
            files={"image": {"data": base64_image, "filename": "uploaded_image.jpg"}}
        )
        st.write(response['choices'][0]['message']['content'])

# Text-to-Speech (TTS) Integration
if st.button("Generate Speech from Text"):
    text_to_speak = st.text_input("Enter text for TTS")
    if text_to_speak:
        response = client.TextToSpeech.create(prompt=text_to_speak, voice="female", model="tts-1-hd")
        audio_url = response['data'][0]['audio_url']
        st.audio(audio_url)

# Assistants API Integration
if st.button("Start Assistant Session"):
    st.session_state["thread_id"] = str(uuid.uuid4())  # Unique thread for the assistant
    assistant_prompt = st.text_area("Enter your query or command for the Assistant")
    if assistant_prompt:
        # Assistant API interaction with persistent thread
        response = client.Assistants.create(
            model="assistant-v2",
            messages=[{"role": "user", "content": assistant_prompt}],
            thread=st.session_state["thread_id"]
        )
        st.write(response['choices'][0]['message']['content'])
